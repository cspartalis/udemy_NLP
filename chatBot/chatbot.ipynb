{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   #specialized format for data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_qa.txt', 'rb') as f:   #read as binary\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_qa.txt', 'rb') as f:   #read as binary\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000 points for trainig data and <br>\n",
    "1000 points for test data <br>\n",
    "each training example is a tuple (story, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating the vocabulary\n",
    "'''\n",
    "all_data = test_data + train_data\n",
    "vocab = set()   #set is an unordered list of unique elements\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "    vocab.add('no')\n",
    "    vocab.add('yes')\n",
    "    \n",
    "vocab_len = len(vocab) + 1   #we will use it later\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We want to the longest story for later use\n",
    "'''\n",
    "all_story_lens = [len(data[0]) for data in all_data]\n",
    "max_story_len = max(all_story_lens)\n",
    "print(max_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We want to the longest question for later use\n",
    "'''\n",
    "all_question_lens = [len(data[1]) for data in all_data]\n",
    "max_question_len = max(all_question_lens)\n",
    "print(max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 1,\n",
       " 'kitchen': 2,\n",
       " 'garden': 3,\n",
       " 'went': 4,\n",
       " 'football': 5,\n",
       " 'picked': 6,\n",
       " '?': 7,\n",
       " 'in': 8,\n",
       " 'took': 9,\n",
       " 'moved': 10,\n",
       " 'put': 11,\n",
       " 'apple': 12,\n",
       " 'mary': 13,\n",
       " 'the': 14,\n",
       " 'dropped': 15,\n",
       " 'is': 16,\n",
       " 'to': 17,\n",
       " '.': 18,\n",
       " 'hallway': 19,\n",
       " 'up': 20,\n",
       " 'bedroom': 21,\n",
       " 'journeyed': 22,\n",
       " 'no': 23,\n",
       " 'milk': 24,\n",
       " 'there': 25,\n",
       " 'travelled': 26,\n",
       " 'got': 27,\n",
       " 'back': 28,\n",
       " 'sandra': 29,\n",
       " 'daniel': 30,\n",
       " 'down': 31,\n",
       " 'bathroom': 32,\n",
       " 'office': 33,\n",
       " 'left': 34,\n",
       " 'discarded': 35,\n",
       " 'grabbed': 36,\n",
       " 'john': 37}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a dictionary that maps every word of the vocabularu to an index\n",
    "'''\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make lists of stories, qustions and answers\n",
    "'''\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answer_text = []\n",
    "\n",
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answer_text.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same for test_data\n",
    "'''\n",
    "test_story_text = []\n",
    "test_question_text = []\n",
    "test_answer_text = []\n",
    "\n",
    "for story, question, answer in test_data:\n",
    "    test_story_text.append(story)\n",
    "    test_question_text.append(question)\n",
    "    test_answer_text.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----CREATE PADDED SEQUENCIES-----\n",
    "VECTORIZING: Conververting words -> indexes,\n",
    "             sentences -> sequencies of integers\n",
    "PADDING: Padding vectors with zeros\n",
    "'''\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n",
    "train_story_seq = pad_sequences(train_story_seq, maxlen=max_story_len)\n",
    "\n",
    "train_question_seq = tokenizer.texts_to_sequences(train_question_text)\n",
    "train_question_seq = pad_sequences(train_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same for test data\n",
    "'''\n",
    "test_story_seq = tokenizer.texts_to_sequences(test_story_text)\n",
    "test_story_seq = pad_sequences(test_story_seq, maxlen=max_story_len)\n",
    "\n",
    "test_question_seq = tokenizer.texts_to_sequences(test_question_text)\n",
    "test_question_seq = pad_sequences(test_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of \"yes\" is: 1\n",
      "Index of \"no\" is:  23\n"
     ]
    }
   ],
   "source": [
    "print(f'Index of \"yes\" is: {tokenizer.word_index.get(\"yes\")}')\n",
    "print(f'Index of \"no\" is:  {tokenizer.word_index.get(\"no\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting train and test answers to one hot representation\n",
    "'''\n",
    "train_answer_oh = np.zeros(shape=(len(train_data), vocab_len))\n",
    "test_answer_oh = np.zeros(shape=(len(train_data), vocab_len))\n",
    "\n",
    "m = len(train_data)   #m is the number of training examples\n",
    "\n",
    "for i in range(m):   #i is the trainining example\n",
    "    train_answer_oh[i][tokenizer.word_index.get(train_answer_text[i])] = 1\n",
    "\n",
    "y = len(test_data)\n",
    "for i in range(y):\n",
    "    test_answer_oh[i][tokenizer.word_index.get(test_answer_text[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input,Activation,Dense,Permute,Dropout,add,dot,concatenate,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER shape=(max_story_len, batch_size)\n",
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT ENCODER M\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_len, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_story_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT ENCODER C\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_len, output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_story_len, max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION ENCODER\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_len, output_dim=64, input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_question_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODED --> ENCODER(INPUT)\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded= question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = dot([input_encoded_m, question_encoded], axes=(2,2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 156, 6)\n",
      "(None, 6, 156)\n"
     ]
    }
   ],
   "source": [
    "response = add([match, input_encoded_c])\n",
    "print(response.shape)\n",
    "response = Permute((2,1))(response)   #changes the axes\n",
    "print(response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 6, 64])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 6, 220])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = concatenate([response, question_encoded])\n",
    "answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = LSTM(units=32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/BiasAdd:0' shape=(None, 38) dtype=float32>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_len)(answer)   #(samples,vocab_size)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_sequence,question], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       multiple             2432        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 6, 64)        2432        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 156, 6)       0           sequential_4[1][0]               \n",
      "                                                                 sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 156, 6)       0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       multiple             228         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 156, 6)       0           activation_3[0][0]               \n",
      "                                                                 sequential_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 6, 156)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6, 220)       0           permute_2[0][0]                  \n",
      "                                                                 sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32)           32384       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 38)           1254        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 38)           0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 2s 209us/step - loss: 0.7455 - accuracy: 0.5094\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 0.7327 - accuracy: 0.4984\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 2s 206us/step - loss: 0.7201 - accuracy: 0.5002\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 2s 221us/step - loss: 0.7111 - accuracy: 0.4975\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.7080 - accuracy: 0.4942\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 3s 274us/step - loss: 0.7039 - accuracy: 0.5052\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 0.7026 - accuracy: 0.4986\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 2s 250us/step - loss: 0.7012 - accuracy: 0.4989\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 3s 260us/step - loss: 0.6996 - accuracy: 0.5052\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.6996 - accuracy: 0.4938\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 0.6987 - accuracy: 0.5010\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 0.6987 - accuracy: 0.4907\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 3s 291us/step - loss: 0.6978 - accuracy: 0.4950\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 3s 269us/step - loss: 0.6976 - accuracy: 0.5026\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 0.6981 - accuracy: 0.4944\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 2s 246us/step - loss: 0.6968 - accuracy: 0.5020\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 3s 268us/step - loss: 0.6972 - accuracy: 0.4976\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 3s 251us/step - loss: 0.6962 - accuracy: 0.4999\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 3s 270us/step - loss: 0.6962 - accuracy: 0.4987\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.6957 - accuracy: 0.5043\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.6965 - accuracy: 0.5021\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 3s 268us/step - loss: 0.6956 - accuracy: 0.5033\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.6958 - accuracy: 0.5000\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 0.6953 - accuracy: 0.5010\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 3s 257us/step - loss: 0.6963 - accuracy: 0.4983\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 0.6954 - accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 3s 271us/step - loss: 0.6949 - accuracy: 0.4951\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 0.6950 - accuracy: 0.4992\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 0.6948 - accuracy: 0.5014\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 3s 260us/step - loss: 0.6951 - accuracy: 0.5007\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.6952 - accuracy: 0.4952\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.6955 - accuracy: 0.4989\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.6946 - accuracy: 0.4984\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 0.6950 - accuracy: 0.4955\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 3s 284us/step - loss: 0.6946 - accuracy: 0.4996\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.6944 - accuracy: 0.5003\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 0.6954 - accuracy: 0.4883\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 3s 297us/step - loss: 0.6947 - accuracy: 0.5050\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 0.6944 - accuracy: 0.5018\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 0.6947 - accuracy: 0.5025\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 3s 251us/step - loss: 0.6948 - accuracy: 0.4984\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.6943 - accuracy: 0.5012\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.6938 - accuracy: 0.5089\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.6937 - accuracy: 0.5084\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.6925 - accuracy: 0.5158\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 3s 264us/step - loss: 0.6888 - accuracy: 0.5283\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.6786 - accuracy: 0.5515\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.6678 - accuracy: 0.5820\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.6638 - accuracy: 0.5951\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 0.6566 - accuracy: 0.5981\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 3s 266us/step - loss: 0.6551 - accuracy: 0.6082\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.6487 - accuracy: 0.6193\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 0.6412 - accuracy: 0.6299\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 3s 276us/step - loss: 0.6392 - accuracy: 0.6375\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.6338 - accuracy: 0.6424\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 2s 225us/step - loss: 0.6255 - accuracy: 0.6503\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 2s 220us/step - loss: 0.6301 - accuracy: 0.6457\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 2s 220us/step - loss: 0.6258 - accuracy: 0.6486\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 2s 225us/step - loss: 0.6226 - accuracy: 0.6507\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 0.6206 - accuracy: 0.6576\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.6199 - accuracy: 0.6561\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 2s 250us/step - loss: 0.6120 - accuracy: 0.6687\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.6128 - accuracy: 0.6654\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 3s 296us/step - loss: 0.6085 - accuracy: 0.6654\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.6027 - accuracy: 0.6806\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 3s 295us/step - loss: 0.5960 - accuracy: 0.6803\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 3s 260us/step - loss: 0.5897 - accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 0.5865 - accuracy: 0.6857\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 3s 263us/step - loss: 0.5830 - accuracy: 0.6916\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 3s 261us/step - loss: 0.5775 - accuracy: 0.6981\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 3s 275us/step - loss: 0.5735 - accuracy: 0.6965\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 3s 271us/step - loss: 0.5692 - accuracy: 0.7026\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 3s 271us/step - loss: 0.5651 - accuracy: 0.7074\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.5630 - accuracy: 0.7046\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 3s 251us/step - loss: 0.5562 - accuracy: 0.7092\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 3s 256us/step - loss: 0.5568 - accuracy: 0.7154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 3s 270us/step - loss: 0.5522 - accuracy: 0.7139\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.5553 - accuracy: 0.7119\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.5502 - accuracy: 0.7137\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.5449 - accuracy: 0.7221\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 0.5433 - accuracy: 0.7204\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.5388 - accuracy: 0.7223\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 3s 278us/step - loss: 0.5351 - accuracy: 0.7260\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 3s 259us/step - loss: 0.5360 - accuracy: 0.7269\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 2s 221us/step - loss: 0.5341 - accuracy: 0.7238\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 2s 227us/step - loss: 0.5277 - accuracy: 0.7302\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.5192 - accuracy: 0.7367\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.5267 - accuracy: 0.7311\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 3s 264us/step - loss: 0.5161 - accuracy: 0.7368\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.5112 - accuracy: 0.7396\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.5045 - accuracy: 0.7469\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 0.5046 - accuracy: 0.7468\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.5016 - accuracy: 0.7483\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 3s 256us/step - loss: 0.4994 - accuracy: 0.7454\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 3s 266us/step - loss: 0.4982 - accuracy: 0.7483\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 0.5013 - accuracy: 0.7464\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 2s 250us/step - loss: 0.4914 - accuracy: 0.7575\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.4896 - accuracy: 0.7527\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 3s 257us/step - loss: 0.4869 - accuracy: 0.7584\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 3s 263us/step - loss: 0.4792 - accuracy: 0.7645\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 0.4794 - accuracy: 0.7641\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 3s 269us/step - loss: 0.4811 - accuracy: 0.7578\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.4742 - accuracy: 0.7641\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 0.4767 - accuracy: 0.7655\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 0.4654 - accuracy: 0.7805\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.4640 - accuracy: 0.7700\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.4665 - accuracy: 0.7727\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 3s 267us/step - loss: 0.4629 - accuracy: 0.7736\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 3s 267us/step - loss: 0.4633 - accuracy: 0.7753\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 2s 223us/step - loss: 0.4666 - accuracy: 0.7779\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.4602 - accuracy: 0.7793\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.4556 - accuracy: 0.7787\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.4538 - accuracy: 0.7817\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.4445 - accuracy: 0.7848\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.4492 - accuracy: 0.7847\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 0.4398 - accuracy: 0.7883\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 3s 279us/step - loss: 0.4424 - accuracy: 0.7925\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.4406 - accuracy: 0.7886\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.4368 - accuracy: 0.7944\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.4268 - accuracy: 0.7998\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.4332 - accuracy: 0.7940\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 0.4258 - accuracy: 0.8013\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.4288 - accuracy: 0.7973\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 3s 268us/step - loss: 0.4205 - accuracy: 0.8032\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.4292 - accuracy: 0.7988\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 3s 263us/step - loss: 0.4277 - accuracy: 0.7996\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 3s 286us/step - loss: 0.4223 - accuracy: 0.8059\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.4263 - accuracy: 0.8013\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 3s 263us/step - loss: 0.4198 - accuracy: 0.8053\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 3s 292us/step - loss: 0.4191 - accuracy: 0.8042\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 3s 288us/step - loss: 0.4211 - accuracy: 0.8018\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.4206 - accuracy: 0.8046\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 3s 270us/step - loss: 0.4076 - accuracy: 0.8140\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 2s 246us/step - loss: 0.4225 - accuracy: 0.8046\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.4140 - accuracy: 0.8107\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.4094 - accuracy: 0.8133\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 3s 270us/step - loss: 0.4056 - accuracy: 0.8152\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.3992 - accuracy: 0.8186\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.4085 - accuracy: 0.8138\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 0.3976 - accuracy: 0.8172\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 3s 264us/step - loss: 0.3968 - accuracy: 0.8174\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.3914 - accuracy: 0.8206\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 0.3991 - accuracy: 0.8134\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 3s 254us/step - loss: 0.3912 - accuracy: 0.8203\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 0.3953 - accuracy: 0.8185\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.3953 - accuracy: 0.8179\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 3s 254us/step - loss: 0.3926 - accuracy: 0.8222\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.3905 - accuracy: 0.8287\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 3s 262us/step - loss: 0.3875 - accuracy: 0.8241\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.3865 - accuracy: 0.8241\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 2s 208us/step - loss: 0.3815 - accuracy: 0.8243\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 309us/step - loss: 0.3892 - accuracy: 0.8253\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 0.3829 - accuracy: 0.8262\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 3s 262us/step - loss: 0.3763 - accuracy: 0.8313\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 3s 260us/step - loss: 0.3805 - accuracy: 0.8270\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 0.3773 - accuracy: 0.8321\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.3789 - accuracy: 0.8284\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 3s 261us/step - loss: 0.3817 - accuracy: 0.8275\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 3s 265us/step - loss: 0.3760 - accuracy: 0.8314\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 3s 264us/step - loss: 0.3697 - accuracy: 0.8313\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.3714 - accuracy: 0.8337\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.3747 - accuracy: 0.8287\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 0.3648 - accuracy: 0.8378\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.3681 - accuracy: 0.8355\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.3720 - accuracy: 0.8304\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 2s 214us/step - loss: 0.3655 - accuracy: 0.8325\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 2s 217us/step - loss: 0.3693 - accuracy: 0.8353\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 2s 230us/step - loss: 0.3607 - accuracy: 0.8365\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 2s 221us/step - loss: 0.3667 - accuracy: 0.8352\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 2s 213us/step - loss: 0.3552 - accuracy: 0.8424\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 2s 230us/step - loss: 0.3576 - accuracy: 0.8416\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 3s 287us/step - loss: 0.3639 - accuracy: 0.8393\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 2s 226us/step - loss: 0.3574 - accuracy: 0.8409\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.3579 - accuracy: 0.8417\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 3s 263us/step - loss: 0.3546 - accuracy: 0.8427\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.3549 - accuracy: 0.8415\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 2s 230us/step - loss: 0.3484 - accuracy: 0.8479\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 3s 266us/step - loss: 0.3611 - accuracy: 0.8424\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.3509 - accuracy: 0.8465\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.3522 - accuracy: 0.8461\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.3546 - accuracy: 0.8448\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 0.3411 - accuracy: 0.8516\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.3429 - accuracy: 0.8454\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.3403 - accuracy: 0.8506\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.3406 - accuracy: 0.8476\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 0.3449 - accuracy: 0.8442\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.3390 - accuracy: 0.8494\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.3397 - accuracy: 0.8492\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.3422 - accuracy: 0.8514\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.3315 - accuracy: 0.8543\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 0.3359 - accuracy: 0.8532\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 0.3305 - accuracy: 0.8521\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 3s 252us/step - loss: 0.3347 - accuracy: 0.8536\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 3s 257us/step - loss: 0.3355 - accuracy: 0.8533\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 3s 294us/step - loss: 0.3308 - accuracy: 0.8542\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 5s 483us/step - loss: 0.3289 - accuracy: 0.8544\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 4s 400us/step - loss: 0.3345 - accuracy: 0.8519\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.3300 - accuracy: 0.8519\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.3245 - accuracy: 0.8574\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 3s 271us/step - loss: 0.3178 - accuracy: 0.8597\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_story_seq, train_question_seq], train_answer_oh, batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'chatbot_120_epochs.h5'\n",
    "filename = 'chatbot_200_epochs.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = load_model('chatbot_200_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accyracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy', 'loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_story_seq, test_question_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 38)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers = []\n",
    "for i in range(len(test_data)):\n",
    "    val_max = np.argmax(predictions[i])\n",
    "    for key, val in tokenizer.word_index.items():\n",
    "        if val == val_max:\n",
    "            predicted_answers.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):   \n",
    "    if predicted_answers[i] == 'yes':\n",
    "        predicted_answers[i] = 1.0\n",
    "    else:\n",
    "        predicted_answers[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):   \n",
    "    if test_answer_text[i] == 'yes':\n",
    "        test_answer_text[i] = 1.0\n",
    "    else:\n",
    "        test_answer_text[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "correct_ans = 0\n",
    "for i in range(len(test_data)):\n",
    "    if predicted_answers[i] == test_answer_text[i]:\n",
    "        correct_ans += 1\n",
    "        \n",
    "accuracy = correct_ans / len(test_data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story = [[]]   #ATTENTION!!! Must be a list of lists\n",
    "my_story[0] = (\"John is in the kitchen.\").split()\n",
    "my_story_seq = tokenizer.texts_to_sequences(my_story)\n",
    "my_story_seq = pad_sequences(my_story_seq, maxlen=max_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = [[]]\n",
    "my_question[0] = ('Is John in the bathroom?').split()\n",
    "my_question_seq = tokenizer.texts_to_sequences(my_question)\n",
    "my_question_seq = pad_sequences(my_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction = model.predict([my_story_seq, my_question_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 38)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_val_max = np.argmax(my_prediction)\n",
    "my_val_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == my_val_max:\n",
    "        k = key\n",
    "print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
