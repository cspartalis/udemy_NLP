{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   #specialized format for data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_qa.txt', 'rb') as f:   #read as binary\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_qa.txt', 'rb') as f:   #read as binary\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000 points for trainig data and <br>\n",
    "1000 points for test data <br>\n",
    "each training example is a tuple (story, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating the vocabulary\n",
    "'''\n",
    "all_data = test_data + train_data\n",
    "vocab = set()   #set is an unordered list of unique elements\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "    vocab.add('no')\n",
    "    vocab.add('yes')\n",
    "    \n",
    "vocab_len = len(vocab) + 1   #we will use it later\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We want to the longest story for later use\n",
    "'''\n",
    "all_story_lens = [len(data[0]) for data in all_data]\n",
    "max_story_len = max(all_story_lens)\n",
    "print(max_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We want to the longest question for later use\n",
    "'''\n",
    "all_question_lens = [len(data[1]) for data in all_data]\n",
    "max_question_len = max(all_question_lens)\n",
    "print(max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hallway': 1,\n",
       " 'bathroom': 2,\n",
       " 'there': 3,\n",
       " 'is': 4,\n",
       " 'back': 5,\n",
       " 'journeyed': 6,\n",
       " 'got': 7,\n",
       " 'to': 8,\n",
       " 'put': 9,\n",
       " 'sandra': 10,\n",
       " 'no': 11,\n",
       " 'office': 12,\n",
       " 'picked': 13,\n",
       " 'moved': 14,\n",
       " 'went': 15,\n",
       " 'discarded': 16,\n",
       " 'grabbed': 17,\n",
       " 'up': 18,\n",
       " 'football': 19,\n",
       " 'kitchen': 20,\n",
       " 'john': 21,\n",
       " 'milk': 22,\n",
       " 'mary': 23,\n",
       " 'apple': 24,\n",
       " 'left': 25,\n",
       " 'the': 26,\n",
       " 'bedroom': 27,\n",
       " 'garden': 28,\n",
       " 'in': 29,\n",
       " 'yes': 30,\n",
       " 'dropped': 31,\n",
       " '?': 32,\n",
       " 'took': 33,\n",
       " 'daniel': 34,\n",
       " '.': 35,\n",
       " 'travelled': 36,\n",
       " 'down': 37}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a dictionary that maps every word of the vocabularu to an index\n",
    "'''\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make lists of stories, qustions and answers\n",
    "'''\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answer_text = []\n",
    "\n",
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answer_text.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same for test_data\n",
    "'''\n",
    "test_story_text = []\n",
    "test_question_text = []\n",
    "test_answer_text = []\n",
    "\n",
    "for story, question, answer in test_data:\n",
    "    test_story_text.append(story)\n",
    "    test_question_text.append(question)\n",
    "    test_answer_text.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----CREATE PADDED SEQUENCIES-----\n",
    "VECTORIZING: Conververting words -> indexes,\n",
    "             sentences -> sequencies of integers\n",
    "PADDING: Padding vectors with zeros\n",
    "'''\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n",
    "train_story_seq = pad_sequences(train_story_seq, maxlen=max_story_len)\n",
    "\n",
    "train_question_seq = tokenizer.texts_to_sequences(train_question_text)\n",
    "train_question_seq = pad_sequences(train_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same for test data\n",
    "'''\n",
    "test_story_seq = tokenizer.texts_to_sequences(test_story_text)\n",
    "test_story_seq = pad_sequences(test_story_seq, maxlen=max_story_len)\n",
    "\n",
    "test_question_seq = tokenizer.texts_to_sequences(test_question_text)\n",
    "test_question_seq = pad_sequences(test_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of \"yes\" is: 30\n",
      "Index of \"no\" is:  11\n"
     ]
    }
   ],
   "source": [
    "print(f'Index of \"yes\" is: {tokenizer.word_index.get(\"yes\")}')\n",
    "print(f'Index of \"no\" is:  {tokenizer.word_index.get(\"no\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting train and test answers to one hot representation\n",
    "'''\n",
    "train_answer_oh = np.zeros(shape=(len(train_data), vocab_len))\n",
    "test_answer_oh = np.zeros(shape=(len(train_data), vocab_len))\n",
    "\n",
    "m = len(train_data)   #m is the number of training examples\n",
    "\n",
    "for i in range(m):   #i is the trainining example\n",
    "    train_answer_oh[i][tokenizer.word_index.get(train_answer_text[i])] = 1\n",
    "\n",
    "y = len(test_data)\n",
    "for i in range(y):\n",
    "    test_answer_oh[i][tokenizer.word_index.get(test_answer_text[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input,Activation,Dense,Permute,Dropout,add,dot,concatenate,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER shape=(max_story_len, batch_size)\n",
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT ENCODER M\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_len, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_story_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT ENCODER C\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_len, output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_story_len, max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION ENCODER\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_len, output_dim=64, input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.33))\n",
    "\n",
    "# OUTPUT\n",
    "# (m, max_question_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODED --> ENCODER(INPUT)\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded= question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = dot([input_encoded_m, question_encoded], axes=(2,2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 156, 6)\n",
      "(None, 6, 156)\n"
     ]
    }
   ],
   "source": [
    "response = add([match, input_encoded_c])\n",
    "print(response.shape)\n",
    "response = Permute((2,1))(response)   #changes the axes\n",
    "print(response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 6, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 6, 220])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = concatenate([response, question_encoded])\n",
    "answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = LSTM(units=32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/BiasAdd:0' shape=(None, 38) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_len)(answer)   #(samples,vocab_size)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_sequence,question], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 5s 545us/step - loss: 1.0383 - accuracy: 0.4897\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 4s 438us/step - loss: 0.7178 - accuracy: 0.5071\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.6986 - accuracy: 0.5063\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 4s 403us/step - loss: 0.6964 - accuracy: 0.4911\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.6946 - accuracy: 0.5012\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 4s 390us/step - loss: 0.6949 - accuracy: 0.4930\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 4s 365us/step - loss: 0.6943 - accuracy: 0.5033\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.6944 - accuracy: 0.5059\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 4s 368us/step - loss: 0.6947 - accuracy: 0.4956\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 4s 363us/step - loss: 0.6947 - accuracy: 0.4985\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 4s 351us/step - loss: 0.6944 - accuracy: 0.5022\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 3s 345us/step - loss: 0.6946 - accuracy: 0.4866\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.6938 - accuracy: 0.4997\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 4s 356us/step - loss: 0.6944 - accuracy: 0.4984\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 4s 369us/step - loss: 0.6934 - accuracy: 0.5048\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.6935 - accuracy: 0.5064\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.6912 - accuracy: 0.5155\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.6716 - accuracy: 0.5839\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 4s 401us/step - loss: 0.5555 - accuracy: 0.7367\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 4s 379us/step - loss: 0.4546 - accuracy: 0.8052\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.4081 - accuracy: 0.8320\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 4s 363us/step - loss: 0.3877 - accuracy: 0.8424\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.3803 - accuracy: 0.8455\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 6s 588us/step - loss: 0.3707 - accuracy: 0.8499\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 4s 437us/step - loss: 0.3602 - accuracy: 0.8522\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.3518 - accuracy: 0.8574\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.3455 - accuracy: 0.8574\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 4s 401us/step - loss: 0.3390 - accuracy: 0.8589\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 5s 493us/step - loss: 0.3366 - accuracy: 0.8589\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 4s 398us/step - loss: 0.3338 - accuracy: 0.8613\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 4s 403us/step - loss: 0.3259 - accuracy: 0.8659\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 4s 387us/step - loss: 0.3241 - accuracy: 0.8652\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 4s 370us/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 4s 438us/step - loss: 0.3174 - accuracy: 0.8614\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.3144 - accuracy: 0.8652\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.3111 - accuracy: 0.8655\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 4s 419us/step - loss: 0.3068 - accuracy: 0.8692\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.3099 - accuracy: 0.8664\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 4s 406us/step - loss: 0.3068 - accuracy: 0.8655\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.3079 - accuracy: 0.8643\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.3056 - accuracy: 0.8673\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 4s 413us/step - loss: 0.3017 - accuracy: 0.8695\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 5s 525us/step - loss: 0.3047 - accuracy: 0.8686\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.2997 - accuracy: 0.8713\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 5s 475us/step - loss: 0.2998 - accuracy: 0.8673\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 4s 438us/step - loss: 0.2996 - accuracy: 0.8683\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 4s 438us/step - loss: 0.2969 - accuracy: 0.8703\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 4s 427us/step - loss: 0.2965 - accuracy: 0.8706\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 4s 414us/step - loss: 0.2967 - accuracy: 0.8691\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 5s 474us/step - loss: 0.2926 - accuracy: 0.8711\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 5s 550us/step - loss: 0.2934 - accuracy: 0.8726\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 5s 505us/step - loss: 0.2891 - accuracy: 0.8720\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 5s 476us/step - loss: 0.2882 - accuracy: 0.8730\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 4s 425us/step - loss: 0.2889 - accuracy: 0.8718\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 5s 489us/step - loss: 0.2897 - accuracy: 0.8709\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 8s 754us/step - loss: 0.2881 - accuracy: 0.8719\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 6s 573us/step - loss: 0.2892 - accuracy: 0.8716\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 7s 654us/step - loss: 0.2853 - accuracy: 0.8736\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 7s 710us/step - loss: 0.2823 - accuracy: 0.8718\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 6s 635us/step - loss: 0.2845 - accuracy: 0.8758\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 6s 630us/step - loss: 0.2823 - accuracy: 0.8778\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 6s 619us/step - loss: 0.2801 - accuracy: 0.8773\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 6s 641us/step - loss: 0.2771 - accuracy: 0.8804\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 4s 436us/step - loss: 0.2785 - accuracy: 0.8776\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 5s 542us/step - loss: 0.2791 - accuracy: 0.8754\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 5s 485us/step - loss: 0.2789 - accuracy: 0.8779\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.2777 - accuracy: 0.8743\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 4s 422us/step - loss: 0.2742 - accuracy: 0.8794\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 4s 430us/step - loss: 0.2734 - accuracy: 0.8796\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 5s 520us/step - loss: 0.2776 - accuracy: 0.8766\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 4s 436us/step - loss: 0.2711 - accuracy: 0.8783\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.2727 - accuracy: 0.8819\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.2695 - accuracy: 0.8828\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 4s 415us/step - loss: 0.2662 - accuracy: 0.8810\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.2642 - accuracy: 0.8855\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.2660 - accuracy: 0.8808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.2665 - accuracy: 0.8816\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 4s 369us/step - loss: 0.2602 - accuracy: 0.8847\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 4s 360us/step - loss: 0.2659 - accuracy: 0.8849\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 0.2619 - accuracy: 0.8833\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2635 - accuracy: 0.8834\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2577 - accuracy: 0.8865\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 4s 379us/step - loss: 0.2563 - accuracy: 0.8889\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2559 - accuracy: 0.8905\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 4s 372us/step - loss: 0.2597 - accuracy: 0.8866\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 0.2612 - accuracy: 0.8855\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.2556 - accuracy: 0.8861\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.2575 - accuracy: 0.8847\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 4s 401us/step - loss: 0.2567 - accuracy: 0.8885\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2586 - accuracy: 0.8854\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 4s 372us/step - loss: 0.2497 - accuracy: 0.8879\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 3s 346us/step - loss: 0.2488 - accuracy: 0.8898\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 4s 358us/step - loss: 0.2492 - accuracy: 0.8916\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 4s 397us/step - loss: 0.2464 - accuracy: 0.8902\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.2462 - accuracy: 0.8911\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.2434 - accuracy: 0.8895\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 4s 437us/step - loss: 0.2444 - accuracy: 0.8928\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 5s 489us/step - loss: 0.2451 - accuracy: 0.8950\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 4s 395us/step - loss: 0.2422 - accuracy: 0.8944\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.2438 - accuracy: 0.8950\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 4s 372us/step - loss: 0.2402 - accuracy: 0.8928\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 4s 365us/step - loss: 0.2417 - accuracy: 0.8941\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.2383 - accuracy: 0.8951\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.2373 - accuracy: 0.8951\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 4s 418us/step - loss: 0.2350 - accuracy: 0.8964\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 4s 413us/step - loss: 0.2371 - accuracy: 0.8980\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 4s 432us/step - loss: 0.2352 - accuracy: 0.8967\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 4s 442us/step - loss: 0.2372 - accuracy: 0.8953\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 4s 435us/step - loss: 0.2328 - accuracy: 0.8970\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 4s 409us/step - loss: 0.2229 - accuracy: 0.9033\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 5s 518us/step - loss: 0.2280 - accuracy: 0.9005\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 4s 422us/step - loss: 0.2326 - accuracy: 0.9002\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 4s 387us/step - loss: 0.2323 - accuracy: 0.8972\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 0.2262 - accuracy: 0.9000\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 4s 371us/step - loss: 0.2233 - accuracy: 0.9039\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 0.2258 - accuracy: 0.9019\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 4s 430us/step - loss: 0.2194 - accuracy: 0.9057\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 4s 395us/step - loss: 0.2259 - accuracy: 0.9004\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 4s 357us/step - loss: 0.2147 - accuracy: 0.9031\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 4s 379us/step - loss: 0.2236 - accuracy: 0.9048\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.2149 - accuracy: 0.9066\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 0.2161 - accuracy: 0.9025\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 4s 383us/step - loss: 0.2181 - accuracy: 0.9063\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 4s 384us/step - loss: 0.2127 - accuracy: 0.9080\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.2143 - accuracy: 0.9058\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.2105 - accuracy: 0.9051\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 4s 407us/step - loss: 0.2149 - accuracy: 0.9068\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2089 - accuracy: 0.9086\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 4s 373us/step - loss: 0.2071 - accuracy: 0.9086\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 4s 395us/step - loss: 0.2067 - accuracy: 0.9104\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.2064 - accuracy: 0.9088\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 4s 408us/step - loss: 0.2022 - accuracy: 0.9124\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 4s 427us/step - loss: 0.2052 - accuracy: 0.9124\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.2005 - accuracy: 0.9161\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 4s 421us/step - loss: 0.1963 - accuracy: 0.9133\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 5s 476us/step - loss: 0.1971 - accuracy: 0.9156\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 5s 451us/step - loss: 0.1957 - accuracy: 0.9182\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 4s 434us/step - loss: 0.1944 - accuracy: 0.9192\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 4s 449us/step - loss: 0.1930 - accuracy: 0.9206\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 4s 387us/step - loss: 0.1914 - accuracy: 0.9207\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 5s 461us/step - loss: 0.1923 - accuracy: 0.9166\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 4s 423us/step - loss: 0.1924 - accuracy: 0.9176\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 4s 434us/step - loss: 0.1852 - accuracy: 0.9233\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 0.1868 - accuracy: 0.9198\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 4s 443us/step - loss: 0.1856 - accuracy: 0.9212\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 5s 490us/step - loss: 0.1914 - accuracy: 0.9195\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 5s 480us/step - loss: 0.1862 - accuracy: 0.9187\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 5s 457us/step - loss: 0.1741 - accuracy: 0.9261\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 4s 418us/step - loss: 0.1784 - accuracy: 0.9259\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 4s 420us/step - loss: 0.1837 - accuracy: 0.9235\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 4s 419us/step - loss: 0.1793 - accuracy: 0.9278\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 422us/step - loss: 0.1753 - accuracy: 0.9241\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.1700 - accuracy: 0.9268\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 4s 359us/step - loss: 0.1735 - accuracy: 0.9256\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 4s 420us/step - loss: 0.1791 - accuracy: 0.9276\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 4s 420us/step - loss: 0.1677 - accuracy: 0.9290\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.1729 - accuracy: 0.9267\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 4s 394us/step - loss: 0.1694 - accuracy: 0.9290\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.1633 - accuracy: 0.9337\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.1633 - accuracy: 0.9328\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 4s 416us/step - loss: 0.1614 - accuracy: 0.9315\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 4s 417us/step - loss: 0.1683 - accuracy: 0.9319\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 4s 409us/step - loss: 0.1633 - accuracy: 0.9297\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 4s 416us/step - loss: 0.1629 - accuracy: 0.9348\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 4s 398us/step - loss: 0.1615 - accuracy: 0.9337\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 4s 409us/step - loss: 0.1521 - accuracy: 0.9357\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.1519 - accuracy: 0.9374\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 4s 412us/step - loss: 0.1537 - accuracy: 0.9383\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.1523 - accuracy: 0.9379\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 4s 400us/step - loss: 0.1598 - accuracy: 0.9365\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 4s 410us/step - loss: 0.1536 - accuracy: 0.9401\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 4s 405us/step - loss: 0.1536 - accuracy: 0.9397\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 4s 420us/step - loss: 0.1531 - accuracy: 0.9385\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 4s 399us/step - loss: 0.1512 - accuracy: 0.9386\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.1461 - accuracy: 0.9432\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 5s 516us/step - loss: 0.1503 - accuracy: 0.9411\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 5s 453us/step - loss: 0.1505 - accuracy: 0.9394\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 5s 509us/step - loss: 0.1483 - accuracy: 0.9419\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 5s 501us/step - loss: 0.1470 - accuracy: 0.9438\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 4s 423us/step - loss: 0.1439 - accuracy: 0.9419\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.1440 - accuracy: 0.9419\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.1387 - accuracy: 0.9453\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 4s 417us/step - loss: 0.1387 - accuracy: 0.9470\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 5s 537us/step - loss: 0.1438 - accuracy: 0.9439\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 6s 598us/step - loss: 0.1432 - accuracy: 0.9446\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 6s 614us/step - loss: 0.1385 - accuracy: 0.9448\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 5s 474us/step - loss: 0.1403 - accuracy: 0.9451\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 6s 555us/step - loss: 0.1399 - accuracy: 0.9461\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 6s 598us/step - loss: 0.1367 - accuracy: 0.9450\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 6s 588us/step - loss: 0.1345 - accuracy: 0.9456\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 6s 552us/step - loss: 0.1324 - accuracy: 0.9493\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 5s 538us/step - loss: 0.1328 - accuracy: 0.9497\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 6s 608us/step - loss: 0.1291 - accuracy: 0.9492\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 6s 554us/step - loss: 0.1297 - accuracy: 0.9494\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 4s 444us/step - loss: 0.1306 - accuracy: 0.9490\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 5s 486us/step - loss: 0.1238 - accuracy: 0.9508\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 5s 485us/step - loss: 0.1255 - accuracy: 0.9511\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 5s 469us/step - loss: 0.1377 - accuracy: 0.9479\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 4s 423us/step - loss: 0.1258 - accuracy: 0.9510\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 4s 417us/step - loss: 0.1312 - accuracy: 0.9493\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_story_seq, train_question_seq], train_answer_oh, batch_size=50, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'chatbot_120_epochs.h5'\n",
    "filename = 'chatbot_200_epochs.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = load_model('chatbot_200_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accyracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy', 'loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_story_seq, test_question_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 38)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers = []\n",
    "for i in range(len(test_data)):\n",
    "    val_max = np.argmax(predictions[i])\n",
    "    for key, val in tokenizer.word_index.items():\n",
    "        if val == val_max:\n",
    "            predicted_answers.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):   \n",
    "    if predicted_answers[i] == 'yes':\n",
    "        predicted_answers[i] = 1.0\n",
    "    else:\n",
    "        predicted_answers[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):   \n",
    "    if test_answer_text[i] == 'yes':\n",
    "        test_answer_text[i] = 1.0\n",
    "    else:\n",
    "        test_answer_text[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865\n"
     ]
    }
   ],
   "source": [
    "correct_ans = 0\n",
    "for i in range(len(test_data)):\n",
    "    if predicted_answers[i] == test_answer_text[i]:\n",
    "        correct_ans += 1\n",
    "        \n",
    "accuracy = correct_ans / len(test_data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story = [[]]   #ATTENTION!!! Must be a list of lists\n",
    "my_story[0] = (\"John is in the kitchen.\").split()\n",
    "my_story_seq = tokenizer.texts_to_sequences(my_story)\n",
    "my_story_seq = pad_sequences(my_story_seq, maxlen=max_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = [[]]\n",
    "my_question[0] = ('Is John in the bathroom?').split()\n",
    "my_question_seq = tokenizer.texts_to_sequences(my_question)\n",
    "my_question_seq = pad_sequences(my_question_seq, maxlen=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction = model.predict([my_story_seq, my_question_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 38)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_val_max = np.argmax(my_prediction)\n",
    "my_val_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == my_val_max:\n",
    "        k = key\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAIL**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
